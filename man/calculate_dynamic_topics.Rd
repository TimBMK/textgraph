% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/calculate_dynamic_topics.R
\name{calculate_dynamic_topics}
\alias{calculate_dynamic_topics}
\title{Calculate Time-Dynamic Topic Clusters}
\usage{
calculate_dynamic_topics(
  data,
  document,
  feature,
  timeframe,
  lookback,
  full_documents = NULL,
  pmi_weight = TRUE,
  negative_edge_weights = TRUE,
  page_rank_calculation = c("global", "cluster", "avg"),
  cluster_function = igraph::cluster_leiden,
  ...,
  seed = TRUE,
  match_clusters = TRUE,
  python_env = "textgraph",
  keep_cluster_objects = FALSE,
  keep_networks = FALSE,
  verbose = TRUE
)
}
\arguments{
\item{data}{Data frame containing the network data.}

\item{document}{String; Name of the document ID column in `data` and (optionally) `full_documents`.}

\item{feature}{String; Name of the feature (e.g. token) column in `data` and (optionally) `full_documents`.}

\item{timeframe}{String; Name of the column indicating the timeframe to base the snapshots on}

\item{lookback}{Numerical; Length of the memory for the matching of snapshot communities. See details.}

\item{full_documents}{Optional, a data frame containing document information. Expects one row per token. NULL to skip.
Document ID and feature columns must have the same names as in `data`. If provided, returns additional information on topic-relevant documents.
Additional variables (such as document meta data) can be passed. Note, however,
that these should contain no more than one unique value per document (see examples).}

\item{pmi_weight}{Logical indicating whether weights should be calculated based on the PMI (Pointwise Mutual Information) of
`document` and `feature`. If `FALSE`, a simple co-occurrence weighting is performed.}

\item{negative_edge_weights}{Logical indicating whether to consider only edges
with positive weights. If the chosen clustering algorithm does not support negative edge weights,
set this to `FALSE`.}

\item{page_rank_calculation}{Character specifying the type of Page Rank calculation.
Options are "global" for global network Page Rank, calculating a full network after the snapshots;
"cluster" for calculating the Page Rank of the entities within each dynamic topic cluster by making subgraphs from the global network (slow, but can be parallelized);
and "avg" for averaging the page rank of each entity over the snapshots (fastest).}

\item{cluster_function}{The clustering function to use. Can be any igraph clustering function.
Default is `igraph::cluster_leiden`.}

\item{...}{Additional arguments passed to `cluster_function`.}

\item{seed}{Seed for the parallelization, if a parallel plan is set up (see details).
TRUE to let future set a seed. Numerical value to set a seed. NULL to not set any seed.}

\item{match_clusters}{Logical. Should the matching algorithm be employed to match the snapshot clusters into temporal topics? Requires
`python_env` if `TRUE`. Setting this to `FALSE` can be helpful to test different clustering settings without running the (potentiall costly)
matching algorithm.}

\item{python_env}{Python environment to use for the dynamic community matching. Set this up with [install_community_matching()]. See details.}

\item{keep_cluster_objects}{Logical indicating whether to keep the igraph cluster object of each snapshot.
Can be helpful for additional checks. See `help(igraph::membership)`. Also returns clustering metrics for each snapshot cluster.}

\item{keep_networks}{Logical indicating whether to keep the igraph network of each snapshot.}

\item{verbose}{Logical indicating whether to print clustering metrics.}
}
\value{
A list object of class "textgraph_topics" containing topic entities,
  document data (optional), clustering metrics, and, optionally, the snapshot data (cluster, networks).
}
\description{
A function to calculate textgraph Topic Clusters dynamically by calculating clusters for each snapshot
 and matching clusters between the snapshots via the Hungarian Method. Note that, depending on network
 structure and lookback, not all entities in the network may end up in a topic.
 Can be parallelized with [future::plan()].
 Requires a Python Environment, preferably installed with [install_community_matching()].
}
\details{
For each snapshot (determined by the `timeframe` variable in `data`), a static cluster is calculated
 via the specified `cluster_function`. Then, these results are matched with the results of adjacent timesteps
 to get a temporal cluster detection. The number previous clusters considered in the matching can be set with `lookback`.
 The matching implementation is Philipp Lorenz' (\href{https://github.com/philipplorenz/memory_community_matching}{Memory Community Matching}).
 As this is a Python implementation, an installation of Python and Reticulate are required. The necessary Python
 environment can then be set up with [install_community_matching()]. If no environment is specified there,
 it conveniently sets up a fresh `textgraph` environment to which `calculate_dynamic_topics()` defaults.

 As the process of calculating snapshots and their clusters can be somewhat time consuming for large networks,
 this function can be parallelized. To make use of parallelization, simply set up a parallelization through the
 `future` package with [future::plan()] - no further action required. If parallelization is set up, the page rank calculation for
 `page_rank_calculation = "cluster"` is parallelized as well. If parallelization is active, it is advised to
 set a seed for the random number generation involved in the clustering. Setting `seed = TRUE` lets `future` set
 a seed, but a custom seed can be provided for reproducability.
}
\examples{
\dontrun{
data("de_pol_twitter")
dynamic_topics <- calculate_dynamic_topics(
  data = de_pol_twitter \%>\%
    dplyr::mutate(day = lubridate::floor_date(created_at, unit = "days")),
  document = "doc_id",
  feature = "lemma",
  timeframe = "day",
  lookback = 3,
  full_documents = NULL,
  pmi_weight = TRUE,
  negative_edge_weights = TRUE,
  page_rank_calculation = "avg",
  cluster_function = igraph::cluster_leiden,
  python_env = "textgraph")

# optionally, we can add a dataframe with document information to get relevant documents
dynamic_topics <- calculate_dynamic_topics(
  data = de_pol_twitter \%>\%
    dplyr::mutate(day = lubridate::floor_date(created_at, unit = "days")), # make a "day" indicator to use as "timeframe"
  document = "doc_id",
  feature = "lemma",
  timeframe = "day",
  lookback = 3,
  full_documents = de_pol_twitter \%>\% # when passing document data...
    dplyr::select(doc_id, lemma, # ...we reduce the columns to the required variables...
                  created_at, author_id, party, tweet_url), #...and document (not token!) metadata
  pmi_weight = TRUE,
  negative_edge_weights = TRUE,
  page_rank_calculation = "avg",
  cluster_function = igraph::cluster_leiden,
  python_env = "textgraph")
}
}
\references{
Lorenz, Philipp et al. 2018. “Capturing the Dynamics of Hashtag-Communities.” In: Complex Networks & Their Applications VI, Studies in Computational Intelligence.
(\href{https://doi.org/10.1007/978-3-319-72150-7_33}{DOI: 10.1007/978-3-319-72150-7_33})
}
